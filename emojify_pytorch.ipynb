{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, num_emojis):\n",
    "    Y = np.eye(num_emojis)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "\n",
    "def read_emoji_csv(path):\n",
    "    data = pd.read_csv(path, header=None)\n",
    "    X = data.iloc[:,0].values\n",
    "    y = data.iloc[:,1].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        for i, w in enumerate(sorted(words), start=1):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m,)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len)).astype(np.float32)\n",
    "    for i in range(m):\n",
    "        sentence_words = [i.lower() for i in X[i].split()]\n",
    "        for idx, val in enumerate(sentence_words):\n",
    "            X_indices[i, idx] = word_to_index[val]\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = len(word_to_index)\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    emb_dim = word_to_vec_map[any_word].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, emb_dim)\n",
    "    embedding_layer.weight.requires_grad = False\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def torch_pretrained_embedding():\n",
    "    glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "    word2idx = glove.stoi\n",
    "    idx2word = glove.itos\n",
    "    embedding_layer = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "    return embedding_layer, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset():\n",
    "    def __init__(self, X_features, y_labels):\n",
    "        self.x_features = torch.from_numpy(X_features)\n",
    "        self.y_labels = torch.from_numpy(y_labels).type(torch.float32)\n",
    "        self.len = len(X_features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_features[index], self.y_labels[index]\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    labels = torch.argmax(labels, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ModelHelperFunctions(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        lines, labels = batch\n",
    "        lines, labels = lines.to(torch.float32).to(torch.device(\"mps\")), labels.to(torch.device(\"mps\"))\n",
    "        output = self(lines)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        return loss\n",
    "\n",
    "    def validate(self, batch):\n",
    "        line, labels = batch\n",
    "        #line = line.reshape(3, sequence_length, embedding_dim)\n",
    "        line, labels = line.to(torch.device(\"mps\")), labels.to(torch.device(\"mps\"))\n",
    "        output = self(line)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        acc = accuracy(output, labels)\n",
    "        return {'test_loss': loss.detach(), 'test_acc': acc}\n",
    "\n",
    "    def validation_results(self, outputs):\n",
    "        batch_losses = [x['test_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['test_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'test_loss': epoch_loss.item(), \n",
    "                'test_acc': epoch_acc.item()}\n",
    "\n",
    "    def result_per_epoch(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, test_loss: {:.4f}, test_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['test_loss'], result['test_acc']))\n",
    "\n",
    "\n",
    "class GRUNet(ModelHelperFunctions):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hidden_dim, \n",
    "                 num_classes, \n",
    "                 num_layers, \n",
    "                 embedding_layer, \n",
    "                 embedding_dim, \n",
    "                 dropout=0.1):\n",
    "\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings = embedding_layer\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True) # x must have shape: (batch_size, seq_len, input_size)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(torch.device(\"mps\"))\n",
    "        x = self.embeddings(x.to(torch.int64))\n",
    "        out, _ = self.gru(x, hidden) # shape: (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_emoji_csv('data/train_emoji.csv')\n",
    "X_test, y_test = read_emoji_csv('data/test_emoji.csv')\n",
    "\n",
    "embedding_layer, word2idx, idx2word = torch_pretrained_embedding()\n",
    "\n",
    "max_length  = len(max(X_train, key=len).split())\n",
    "X_train_indices = sentences_to_indices(X_train, word2idx, max_length)\n",
    "X_test_indices = sentences_to_indices(X_test, word2idx, max_length)\n",
    "y_train_oh = convert_to_one_hot(y_train, num_emojis = 30)\n",
    "y_test_oh = convert_to_one_hot(y_test, num_emojis = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['how are you doing' 'I am really busy at the moment'\n",
      " 'we can take it from here']\n",
      "X1_indices =\n",
      " [[ 197.   32.   81.  914.    0.    0.    0.]\n",
      " [  41.  913.  588. 4259.   22.    0. 1600.]\n",
      " [  53.   86.  190.   20.   25.  187.    0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"how are you doing\", \"I am really busy at the moment\", \"we can take it from here\"])\n",
    "X1_indices = sentences_to_indices(X1, word2idx, max_len=7)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "sequence_length = max_length\n",
    "hidden_size = 128\n",
    "num_classes = 30\n",
    "batch_size = 3\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "num_layers = 2\n",
    "\n",
    "model = GRUNet(input_dim=sequence_length, \n",
    "               hidden_dim=hidden_size,\n",
    "               num_classes=num_classes,\n",
    "               num_layers=num_layers,\n",
    "               embedding_layer=embedding_layer,\n",
    "               embedding_dim=embedding_dim).to(torch.device(\"mps\"))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validate(batch) for batch in test_loader]\n",
    "    return model.validation_results(outputs)\n",
    "\n",
    "\n",
    "def fit(model, num_epochs, train_loader, test_loader, lr, optimizer):\n",
    "    history = []\n",
    "    epoch_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_list.append(epoch)\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Testing Phase\n",
    "        result = evaluate(model, test_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.result_per_epoch(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history, epoch_list\n",
    "\n",
    "\n",
    "def plot_results(history, epoch_lst):\n",
    "    accuracies = [x['test_acc'] for x in history]\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['test_loss'] for x in history]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    ax1.plot(epoch_lst, accuracies)\n",
    "    ax1.set_title('Validation Accuracy vs. No. of epochs')\n",
    "    ax1.set(ylabel='Accuracy')\n",
    "    ax2.plot(epoch_lst, train_losses)\n",
    "    ax2.plot(epoch_lst, val_losses)\n",
    "    ax2.set_title('Losses vs. No. of epochs')\n",
    "    ax2.set(xlabel='Epochs', ylabel='Loss')\n",
    "    ax2.legend(['Training', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines in training data: 189\n",
      "Lines in testing data: 54\n"
     ]
    }
   ],
   "source": [
    "train_data = EmojiDataset(X_features=X_train_indices, y_labels=y_train_oh)\n",
    "test_data = EmojiDataset(X_features=X_test_indices, y_labels=y_test_oh)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "print(\"Lines in training data:\", len(train_loader.dataset))\n",
    "print(\"Lines in testing data:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 3.4234, test_loss: 3.3385, test_acc: 0.1296\n",
      "Epoch [1], train_loss: 3.3517, test_loss: 3.3798, test_acc: 0.0370\n",
      "Epoch [2], train_loss: 3.2723, test_loss: 3.3528, test_acc: 0.0926\n",
      "Epoch [3], train_loss: 3.1368, test_loss: 3.1412, test_acc: 0.1852\n",
      "Epoch [4], train_loss: 2.8907, test_loss: 2.8723, test_acc: 0.2407\n",
      "Epoch [5], train_loss: 2.4894, test_loss: 2.8212, test_acc: 0.2778\n",
      "Epoch [6], train_loss: 2.2429, test_loss: 2.7424, test_acc: 0.2963\n",
      "Epoch [7], train_loss: 1.9578, test_loss: 2.7710, test_acc: 0.2407\n",
      "Epoch [8], train_loss: 1.7511, test_loss: 3.0040, test_acc: 0.2778\n",
      "Epoch [9], train_loss: 1.6186, test_loss: 2.9047, test_acc: 0.2593\n",
      "Epoch [10], train_loss: 1.3700, test_loss: 2.8181, test_acc: 0.2963\n",
      "Epoch [11], train_loss: 1.1644, test_loss: 2.7615, test_acc: 0.3333\n",
      "Epoch [12], train_loss: 0.9918, test_loss: 2.9186, test_acc: 0.2778\n",
      "Epoch [13], train_loss: 0.7648, test_loss: 3.0563, test_acc: 0.2593\n",
      "Epoch [14], train_loss: 0.5746, test_loss: 3.1308, test_acc: 0.2778\n",
      "Epoch [15], train_loss: 0.4393, test_loss: 3.1505, test_acc: 0.3333\n",
      "Epoch [16], train_loss: 0.3956, test_loss: 3.2767, test_acc: 0.3333\n",
      "Epoch [17], train_loss: 0.3248, test_loss: 3.2556, test_acc: 0.3148\n",
      "Epoch [18], train_loss: 0.2288, test_loss: 3.3584, test_acc: 0.3704\n",
      "Epoch [19], train_loss: 0.1881, test_loss: 3.3847, test_acc: 0.3519\n",
      "Epoch [20], train_loss: 0.1233, test_loss: 3.4617, test_acc: 0.3519\n",
      "Epoch [21], train_loss: 0.0969, test_loss: 3.3566, test_acc: 0.3889\n",
      "Epoch [22], train_loss: 0.0692, test_loss: 3.4861, test_acc: 0.3519\n",
      "Epoch [23], train_loss: 0.0560, test_loss: 3.5366, test_acc: 0.3704\n",
      "Epoch [24], train_loss: 0.0512, test_loss: 3.5740, test_acc: 0.3519\n"
     ]
    }
   ],
   "source": [
    "history, epoch_lst = fit(model=model, \n",
    "                         num_epochs=num_epochs, \n",
    "                         train_loader=train_loader,\n",
    "                         test_loader=test_loader,\n",
    "                         lr=learning_rate,\n",
    "                         optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emojify_env",
   "language": "python",
   "name": "emojify_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
